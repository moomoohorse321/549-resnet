"""
The following sccript is written with the help of Github Copilot (GPT-4.1 mini)
The printing and plotting is generated by Claude Opus-4.5, with the context being the implementation and our intent (plot / print). 
The code is debugged with the help of Claude Opus-4.5, with the context being error logs, intermediate plots and code snippets.

Note that the data in torch vision are broken. 

You should scrape the data to data directory manually. Because of the size of data, we didn't submit the data files. Contact Hao (haor2@illinois.edu) if you want to have a copy of the data.
"""

import os
import torch
import torch.utils.data
from torch.optim import SGD
from torch.optim.lr_scheduler import MultiStepLR
import torchvision
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.models.detection.backbone_utils import resnet_fpn_backbone
from torchvision.datasets import VOCDetection
import torchvision.transforms.v2 as T
from tqdm import tqdm
import argparse
from collections import defaultdict
import numpy as np


VOC_CLASSES = [
    '__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',
    'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',
    'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
]
CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(VOC_CLASSES)}


class VOCDataset(torch.utils.data.Dataset):
    """PASCAL VOC dataset wrapper for Faster R-CNN."""
    
    def __init__(self, root, year, image_set, transforms=None):
        # Changed download=False to avoid crashes if URLs are broken
        # Ensure you extract the manual downloads to args.data_root
        self.dataset = VOCDetection(
            root=root, year=year, image_set=image_set, download=False
        )
        self.transforms = transforms
    
    def __len__(self):
        return len(self.dataset)
    
    def __getitem__(self, idx):
        img, target = self.dataset[idx]
        
        # Parse VOC annotation
        annotation = target['annotation']
        objects = annotation.get('object', [])
        if not isinstance(objects, list):
            objects = [objects]
        
        boxes = []
        labels = []
        
        for obj in objects:
            if obj.get('difficult', '0') == '1':
                continue  # Skip difficult objects during training
            
            bbox = obj['bndbox']
            xmin = float(bbox['xmin'])
            ymin = float(bbox['ymin'])
            xmax = float(bbox['xmax'])
            ymax = float(bbox['ymax'])
            
            # Validate box
            if xmax > xmin and ymax > ymin:
                boxes.append([xmin, ymin, xmax, ymax])
                labels.append(CLASS_TO_IDX[obj['name']])
        
        if len(boxes) == 0:
            # Handle images with no valid objects
            boxes = torch.zeros((0, 4), dtype=torch.float32)
            labels = torch.zeros((0,), dtype=torch.int64)
        else:
            boxes = torch.as_tensor(boxes, dtype=torch.float32)
            labels = torch.as_tensor(labels, dtype=torch.int64)
        
        target_dict = {
            'boxes': boxes,
            'labels': labels,
            'image_id': torch.tensor([idx]),
        }
        
        # Convert PIL to tensor
        img = T.functional.to_image(img)
        img = T.functional.to_dtype(img, dtype=torch.float32, scale=True)
        
        if self.transforms is not None:
            img, target_dict = self.transforms(img, target_dict)
        
        return img, target_dict


class Compose:
    def __init__(self, transforms):
        self.transforms = transforms
    
    def __call__(self, img, target):
        for t in self.transforms:
            img, target = t(img, target)
        return img, target


class RandomHorizontalFlip:
    def __init__(self, prob=0.5):
        self.prob = prob
    
    def __call__(self, img, target):
        if torch.rand(1) < self.prob:
            img = T.functional.horizontal_flip(img)
            if 'boxes' in target and len(target['boxes']) > 0:
                _, _, width = img.shape
                boxes = target['boxes']
                boxes[:, [0, 2]] = width - boxes[:, [2, 0]]
                target['boxes'] = boxes
        return img, target


class Normalize:
    def __init__(self, mean, std):
        self.mean = mean
        self.std = std
    
    def __call__(self, img, target):
        img = T.functional.normalize(img, mean=self.mean, std=self.std)
        return img, target


def get_transforms(train=True):
    transforms = []
    if train:
        transforms.append(RandomHorizontalFlip(0.5))
    transforms.append(Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    ))
    return Compose(transforms)

def build_model(num_classes=21, pretrained_backbone=True):
    """Build Faster R-CNN with ResNet-101-FPN backbone."""
    
    backbone = resnet_fpn_backbone(
        backbone_name='resnet101',
        weights='ResNet101_Weights.IMAGENET1K_V1' if pretrained_backbone else None,
        trainable_layers=3  # Reduced from 5 to 3 to save memory/prevent overfitting
    )
    
    anchor_sizes = ((32,), (64,), (128,), (256,), (512,))
    aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)
    anchor_generator = AnchorGenerator(sizes=anchor_sizes, aspect_ratios=aspect_ratios)
    
    model = FasterRCNN(
        backbone,
        num_classes=num_classes,
        rpn_anchor_generator=anchor_generator,
        min_size=600,
        max_size=1000,
        box_detections_per_img=100,
    )
    
    return model

def compute_iou(box1, box2):
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    inter = max(0, x2 - x1) * max(0, y2 - y1)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - inter
    
    return inter / union if union > 0 else 0


def compute_ap(recalls, precisions):
    ap = 0.0
    for t in np.arange(0, 1.1, 0.1):
        precisions_above = [p for r, p in zip(recalls, precisions) if r >= t]
        p = max(precisions_above) if precisions_above else 0
        ap += p / 11
    return ap


@torch.no_grad()
def evaluate_voc(model, dataloader, device, iou_threshold=0.5):
    model.eval()
    
    all_predictions = defaultdict(list)
    all_ground_truths = defaultdict(list)
    
    for images, targets in tqdm(dataloader, desc="Evaluating"):
        images = [img.to(device) for img in images]
        outputs = model(images)
        
        for target, output in zip(targets, outputs):
            img_id = target['image_id'].item()
            
            gt_boxes = target['boxes'].numpy()
            gt_labels = target['labels'].numpy()
            for box, label in zip(gt_boxes, gt_labels):
                if label > 0:
                    all_ground_truths[label].append((img_id, box, False))
            
            pred_boxes = output['boxes'].cpu().numpy()
            pred_labels = output['labels'].cpu().numpy()
            pred_scores = output['scores'].cpu().numpy()
            for box, label, score in zip(pred_boxes, pred_labels, pred_scores):
                if label > 0:
                    all_predictions[label].append((img_id, score, box))
    
    aps = []
    for cls in range(1, 21):
        preds = all_predictions[cls]
        gts = all_ground_truths[cls]
        
        if len(gts) == 0:
            continue
        
        preds = sorted(preds, key=lambda x: x[1], reverse=True)
        gt_detected = {i: False for i in range(len(gts))}
        
        tp = []
        fp = []
        
        for img_id, score, pred_box in preds:
            best_iou = 0
            best_gt_idx = -1
            
            for gt_idx, (gt_img_id, gt_box, _) in enumerate(gts):
                if gt_img_id != img_id:
                    continue
                iou = compute_iou(pred_box, gt_box)
                if iou > best_iou:
                    best_iou = iou
                    best_gt_idx = gt_idx
            
            if best_iou >= iou_threshold and not gt_detected.get(best_gt_idx, True):
                tp.append(1)
                fp.append(0)
                gt_detected[best_gt_idx] = True
            else:
                tp.append(0)
                fp.append(1)
        
        tp_cumsum = np.cumsum(tp)
        fp_cumsum = np.cumsum(fp)
        recalls = tp_cumsum / len(gts)
        precisions = tp_cumsum / (tp_cumsum + fp_cumsum)
        
        ap = compute_ap(recalls, precisions)
        aps.append(ap)
    
    mAP = np.mean(aps) if aps else 0
    return mAP * 100

def train_one_epoch(model, dataloader, optimizer, device, epoch, accumulation_steps=4):
    """Train for one epoch with CORRECT gradient accumulation."""
    model.train()
    
    # 1. Zero gradients BEFORE the loop starts
    optimizer.zero_grad()
    
    total_loss = 0
    num_batches = 0
    
    progress = tqdm(dataloader, desc=f"Epoch {epoch}")
    for batch_idx, (images, targets) in enumerate(progress):
        images = [img.to(device) for img in images]
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
        
        # Forward pass
        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())
        
        # Scale loss
        loss_scaled = losses / accumulation_steps
        loss_scaled.backward()
        
        # 2. Step and Zero ONLY after accumulation steps
        if (batch_idx + 1) % accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()
        
        total_loss += losses.item()
        num_batches += 1
        
        progress.set_postfix(loss=f"{total_loss/num_batches:.4f}")
    
    # 3. Apply remaining gradients if dataset isn't divisible by steps
    if (batch_idx + 1) % accumulation_steps != 0:
        optimizer.step()
        optimizer.zero_grad()
    
    return total_loss / num_batches


def collate_fn(batch):
    return tuple(zip(*batch))

def main():
    parser = argparse.ArgumentParser(description='Train Faster R-CNN on PASCAL VOC')
    parser.add_argument('--data_root', type=str, default='./data/VOC',
                        help='Root directory for VOC dataset')
    parser.add_argument('--epochs', type=int, default=12,
                        help='Number of training epochs')
    parser.add_argument('--lr', type=float, default=0.005,
                        help='Learning rate')
    parser.add_argument('--accumulation_steps', type=int, default=4,
                        help='Gradient accumulation steps')
    parser.add_argument('--eval_only', action='store_true',
                        help='Only run evaluation')
    parser.add_argument('--checkpoint', type=str, default=None,
                        help='Path to checkpoint for Resume or Eval')
    parser.add_argument('--device', type=str, default='cuda',
                        help='Device to use')
    args = parser.parse_args()
    
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    os.makedirs(args.data_root, exist_ok=True)
    os.makedirs("checkpoints", exist_ok=True)
    
    if not args.eval_only:
        print("\n=== Loading training data (VOC 2007+2012) ===")
        
        # NOTE: download=False to avoid URL errors. Ensure files are in data_root.
        train_dataset_07 = VOCDataset(
            root=args.data_root, year='2007', image_set='trainval',
            transforms=get_transforms(train=True)
        )
        train_dataset_12 = VOCDataset(
            root=args.data_root, year='2012', image_set='trainval',
            transforms=get_transforms(train=True)
        )
        
        train_dataset = torch.utils.data.ConcatDataset([train_dataset_07, train_dataset_12])
        
        train_loader = torch.utils.data.DataLoader(
            train_dataset, batch_size=1, shuffle=True,
            num_workers=2, collate_fn=collate_fn, pin_memory=True
        )
        
        print(f"Training samples: {len(train_dataset)}")
        
        print("\n=== Building Model ===")
        model = build_model(num_classes=21, pretrained_backbone=True)
        model.to(device)
        
        params = [p for p in model.parameters() if p.requires_grad]
        optimizer = SGD(params, lr=args.lr, momentum=0.9, weight_decay=0.0001)
        scheduler = MultiStepLR(optimizer, milestones=[9], gamma=0.1)
        
        start_epoch = 1

        if args.checkpoint and os.path.exists(args.checkpoint):
            print(f"Resuming training from checkpoint: {args.checkpoint}")
            checkpoint = torch.load(args.checkpoint, map_location=device)
            
            # Load model
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint) # fallback for old checkpoints
            
            # Load optimizer & scheduler if available
            if 'optimizer_state_dict' in checkpoint:
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            if 'scheduler_state_dict' in checkpoint:
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            if 'epoch' in checkpoint:
                start_epoch = checkpoint['epoch'] + 1
                print(f"Resuming from Epoch {start_epoch}")

        print("\n=== Starting training ===")
        for epoch in range(start_epoch, args.epochs + 1):
            avg_loss = train_one_epoch(
                model, train_loader, optimizer, device, epoch,
                accumulation_steps=args.accumulation_steps
            )
            scheduler.step()
            
            current_lr = scheduler.get_last_lr()[0]
            print(f"Epoch {epoch} - Avg Loss: {avg_loss:.4f}, LR: {current_lr:.6f}")
            
            # Save Checkpoint (Full State)
            if epoch % 1 == 0: # Save every epoch just in case
                ckpt_path = f"checkpoints/fasterrcnn_resnet101_voc_epoch{epoch}.pth"
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict()
                }, ckpt_path)
                print(f"Saved: {ckpt_path}")

    else:
        print("\n=== Evaluating on VOC 2007 test ===")
        model = build_model(num_classes=21, pretrained_backbone=False)
        
        if args.checkpoint:
            print(f"Loading weights from {args.checkpoint}")
            checkpoint = torch.load(args.checkpoint, map_location=device)
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint)
        
        model.to(device)
        
        test_dataset = VOCDataset(
            root=args.data_root, year='2007', image_set='test',
            transforms=get_transforms(train=False)
        )
        test_loader = torch.utils.data.DataLoader(
            test_dataset, batch_size=1, shuffle=False,
            num_workers=2, collate_fn=collate_fn
        )
        
        mAP = evaluate_voc(model, test_loader, device)
        print(f"\n{'='*50}")
        print(f"VOC 2007 Test mAP@0.5: {mAP:.1f}%")
        print(f"{'='*50}")


if __name__ == "__main__":
    main()